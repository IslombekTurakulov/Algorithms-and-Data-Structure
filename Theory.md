# Подготовка к экзамену по предмету Алгоритмы и структуры данных (АиСД)

1. Понятие алгоритма. Алгоритмические модели. Машина с произвольным доступом к памяти. Основные характеристики алгоритмов. Понятие абстрактного типа данных. 

    Алгоритм — это корректно определенная вычислительная процедура, представляющая собой конечною последовательность действий, на вход которой подается некоторое слово и результатом выполнения которой является словом.
    
    Машину с произвольным доступом к памяти (Random Access Machine, RAM, Равновероятная адресная машина) можно рассматривать как компьютер, работающий следующим образом:
    
        • алгоритм состоит из конечного числа команд; 
        
        • для исполнения простой операции {+,∗,−,=,if,...} требуется один шаг; 
        
        • каждое обращение к памяти занимает один шаг; 
        
        • объем памяти неограничен; 
        
        • один шаг выполняется за одну единицу времени; 
        
        • любой алгоритм может быть представлен в виде слова над некоторым алфавитом. Мы будем считать, что алгоритм записан на некотором языке программирования.
    
    Характеристики алгоритма: 
    
        • Корректность. 
        
        • Сложность. 
        
        • Требования к памяти. 
        
        • Способность к распараллеливанию
        
    Абстра́ктный тип да́нных (АТД) — это математическая модель для типов данных, где тип данных определяется поведением (семантикой) с точки зрения пользователя данных, а именно в терминах возможных значений, возможных операций над данными этого типа и поведения этих операций.

2. Сложность алгоритмов. Асимптотические обозначения. Оценки сложности. 

![Сложность](https://i.yapx.ru/KiOCF.png)

![Оценки](https://i.yapx.ru/KiOKP.png)

![Ассимптотические обозначения](https://i.yapx.ru/KiOLr.png)

![Свойства](https://i.yapx.ru/KiOMO.png)

3. Линейные структуры данных. Динамические массивы. Стеки и очереди. Связные списки.

    Любой алгоритм использует данные. Важно эти данные хранить
    так, чтобы алгоритмы работали достаточно эффективно.
    Не существует одной — наилучшей структуры данных. В каких-то
    случаях более подходящей является одна структура данных, в
    других случаях — другая.

    Основные операции над данными:
        • добавление элемента;

        • поиск;

        • удаление элемента.

    Характеристики структуры данных:

        • сложность основных операций;

        • требования к памяти.
    
    [Структуры и свойства](https://docviewer.yandex.ru/view/439651512/?*=vsvVdngJzjarin%2B6CTPqEkjuh7t7InVybCI6InlhLWRpc2stcHVibGljOi8vdG80dTd1T3dEOU9HMUI1aDg5bUcvbmgzdVU5Q0FxMUswVkpBbXR1dEZPRXRjSjdjNEFsMU81eFFxSHJ2YzU5VXEvSjZicG1SeU9Kb25UM1ZvWG5EYWc9PTov0KHQtdC80LXRgdGC0YAgNS%2FQkNC70LPQvtGA0LjRgtC80Ysg0Lgg0YHRgtGA0YPQutGC0YPRgNGLINC00LDQvdC90YvRhS%2FQm9C10LrRhtC40Lgg0YHQu9Cw0LnQtNGLL0FsZ29yaXRobXMgKDIpLnBkZiIsInRpdGxlIjoiQWxnb3JpdGhtcyAoMikucGRmIiwibm9pZnJhbWUiOmZhbHNlLCJ1aWQiOiI0Mzk2NTE1MTIiLCJ0cyI6MTYxMDc4MjkxMjE5MSwieXUiOiIxNjUxMTQ0ODgxNTg0OTU1MjI3In0%3D)

4. Хэш-таблицы. Хэш-функции. 

Хеш-таблица — это структура данных, реализующая интерфейс ассоциативного массива, а именно, она позволяет хранить пары (ключ, значение) и выполнять три операции: операцию добавления новой пары, операцию поиска и операцию удаления пары по ключу.

Существуют два основных варианта хеш-таблиц: с цепочками и открытой адресацией. Хеш-таблица содержит некоторый массив H, элементы которого есть пары (хеш-таблица с открытой адресацией) или списки пар (хеш-таблица с цепочками).

Выполнение операции в хеш-таблице начинается с вычисления хеш-функции от ключа. Получающееся хеш-значение i=hash(key) играет роль индекса в массиве H. Затем выполняемая операция (добавление, удаление или поиск) перенаправляется объекту, который хранится в соответствующей ячейке массива H[i].

Ситуация, когда для различных ключей получается одно и то же хеш-значение, называется коллизией. Такие события не так уж и редки — например, при вставке в хеш-таблицу размером 365 ячеек всего лишь 23 элементов вероятность коллизии уже превысит 50 % (если каждый элемент может равновероятно попасть в любую ячейку) — см. парадокс дней рождения. Поэтому механизм разрешения коллизий — важная составляющая любой хеш-таблицы.

В некоторых специальных случаях удаётся избежать коллизий вообще. Например, если все ключи элементов известны заранее (или очень редко меняются), то для них можно найти некоторую совершенную хеш-функцию, которая распределит их по ячейкам хеш-таблицы без коллизий. Хеш-таблицы, использующие подобные хеш-функции, не нуждаются в механизме разрешения коллизий, и называются хеш-таблицами с прямой адресацией.

Число хранимых элементов, делённое на размер массива H (число возможных значений хеш-функции), называется коэффициентом заполнения хеш-таблицы (load factor) и является важным параметром, от которого зависит среднее время выполнения операций.

Важное свойство хеш-таблиц состоит в том, что, при некоторых разумных допущениях, все три операции (поиск, вставка, удаление элементов) в среднем выполняются за время O(1). Но при этом не гарантируется, что время выполнения отдельной операции мало́. Это связано с тем, что при достижении некоторого значения коэффициента заполнения необходимо осуществлять перестройку индекса хеш-таблицы: увеличить значение размера массива H и заново добавить в пустую хеш-таблицу все пары.

### Метод цепочек

Разрешение коллизий при помощи цепочек.
Каждая ячейка массива H является указателем на связный список (цепочку) пар ключ-значение, соответствующих одному и тому же хеш-значению ключа. Коллизии просто приводят к тому, что появляются цепочки длиной более одного элемента.

Операции поиска или удаления элемента требуют просмотра всех элементов соответствующей ему цепочки, чтобы найти в ней элемент с заданным ключом. Для добавления элемента нужно добавить элемент в конец или начало соответствующего списка и в случае, если коэффициент заполнения станет слишком велик, увеличить размер массива H и перестроить таблицу.

При предположении, что каждый элемент может попасть в любую позицию таблицы H с равной вероятностью и независимо от того, куда попал любой другой элемент, среднее время работы операции поиска элемента составляет Θ(1 + α), где α — коэффициент заполнения таблицы.

### Открытая адресация

Пример хеш-таблицы с открытой адресацией и линейным пробированием, получающейся при вставке элементов в левой колонке сверху вниз.
В массиве H хранятся сами пары ключ-значение. Алгоритм вставки элемента проверяет ячейки массива H в некотором порядке до тех пор, пока не будет найдена первая свободная ячейка, в которую и будет записан новый элемент. Этот порядок вычисляется на лету, что позволяет сэкономить на памяти для указателей, требующихся в хеш-таблицах с цепочками.

Последовательность, в которой просматриваются ячейки хеш-таблицы, называется последовательностью проб. В общем случае, она зависит только от ключа элемента, то есть это последовательность h0(x), h1(x), …, hn — 1(x), где x — ключ элемента, а hi(x) — произвольные функции, сопоставляющие каждому ключу ячейку в хеш-таблице. Первый элемент в последовательности, как правило, равен значению некоторой хеш-функции от ключа, а остальные считаются от него одним из приведённых ниже способов. Для успешной работы алгоритмов поиска последовательность проб должна быть такой, чтобы все ячейки хеш-таблицы оказались просмотренными ровно по одному разу.

Алгоритм поиска просматривает ячейки хеш-таблицы в том же самом порядке, что и при вставке, до тех пор, пока не найдется либо элемент с искомым ключом, либо пока не будет достигнут конец списка. Ошибочно представление, что следует искать до первой свободной ячейки, так как возможно, что элемент из этой ячейки был удален, а искомый элемент находится далее.

Удаление элементов в такой схеме несколько затруднено. Обычно поступают так: заводят булевый флаг для каждой ячейки, помечающий, удален элемент в ней или нет. Тогда удаление элемента состоит в установке этого флага для соответствующей ячейки хеш-таблицы, но при этом необходимо модифицировать процедуру поиска существующего элемента так, чтобы она считала удалённые ячейки занятыми, а процедуру добавления — чтобы она их считала свободными и сбрасывала значение флага при добавлении

5. Двоичные деревья поиска. Основные операции над двоичными деревьями поиска.  

Дерево (без выделенного корня) — это связный ациклический
неориентированный граф.

Дерево с корнем, или корневое дерево — дерево, в котором одна из
вершин выделена. Выделенную вершину будем называть корнем.
Вершины корневого дерева называются также узлами (node).

Двоичное дерево — это конечный набор вершин, который либо пуст,
либо разбит на три непересекающиеся части:

    • вершину, называемую корнем (root);
    
    • двоичное дерево, называемое левым поддеревом корня;
    
    • двоичное дерево, называемое правым поддеревом корня.
    
Двоичное дерево, не содержащее вершин, будем называть пустым
и обозначать nil. Если левое поддерево непусто, то его корень
называется левым ребёнком корня всего дерева; правый ребёнок
определяется аналогично.

Представление узла дерева
Узел представляет собой структуру со следующими полями:

    • Ключ key.
    
    • Дополнительные данные.
    
    • Указатель на левого потомка left.
    
    • Указатель на правого потомка right.
    
    • Указатель на родителя p.

Ключи в двоичном дереве поиска хранятся так, чтобы соблюдалось
свойство упорядоченности: Пусть x — произвольная вершина
двоичного дерева поиска. Если вершина y находится в левом
поддереве вершины x, то y.key ≤ x.key.
Если y находится в правом поддереве x, то y.key ≥ x.key.

[Основные операции](https://docviewer.yandex.ru/view/439651512/?page=13&*=t1blMuMfMp9yD4yPnAFISccRN5x7InVybCI6InlhLWRpc2stcHVibGljOi8vdG80dTd1T3dEOU9HMUI1aDg5bUcvbmgzdVU5Q0FxMUswVkpBbXR1dEZPRXRjSjdjNEFsMU81eFFxSHJ2YzU5VXEvSjZicG1SeU9Kb25UM1ZvWG5EYWc9PTov0KHQtdC80LXRgdGC0YAgNS%2FQkNC70LPQvtGA0LjRgtC80Ysg0Lgg0YHRgtGA0YPQutGC0YPRgNGLINC00LDQvdC90YvRhS%2FQm9C10LrRhtC40Lgg0YHQu9Cw0LnQtNGLL0FsZ29yaXRobXMgKDMpLnBkZiIsInRpdGxlIjoiQWxnb3JpdGhtcyAoMykucGRmIiwibm9pZnJhbWUiOmZhbHNlLCJ1aWQiOiI0Mzk2NTE1MTIiLCJ0cyI6MTYxMDc4NDQzMTE3NCwieXUiOiIxNjUxMTQ0ODgxNTg0OTU1MjI3In0%3D)

6. Красно-черные деревья. Основные операции над красно-черными деревьями. 

Красно-чёрное дерево (red-black tree) — это двоичное дерево поиска,
вершины которого разделены на красные (red) и чёрные (black).
Таким образом, каждая вершина хранит один дополнительный
бит — её цвет.

Каждая вершина красно-чёрного дерева имеет поля color (цвет), key (ключ), left (левый ребёнок), right (правый ребёнок) и p (родитель).
Если у вершины отсутствует ребёнок или родитель, соответствующее поле содержит nil.
Будем считать, что значения nil, хранящиеся в полях left и right, являются ссылками на фиктивные листья дерева. Каждая содержащая ключ вершина имеет двух детей.

Потребуем, чтобы красно-чёрное дерево удовлетворяло следующим
свойствам:

    • Каждая вершина — либо красная, либо чёрная.
    
    • Каждый лист — чёрный.
    
    • Оба ребенка красной вершины — чёрные.
    
    • Все пути, идущие вниз от корня к листьям, содержат одинаковое количество чёрных вершин.
    
    •Корень дерева всегда чёрный

Красно-чёрное дерево с n внутренними вершинами имеет высоту
не больше 2 log2(n+ 1).

Доказательство.
Обозначим высоту дерева через h. Согласно свойствам красно-чёрного дерева, не менее половины вершин на пути от корня к листу, не считая корень, составляют чёрные вершины. Поэтому, чёрная высота дерева не меньше h/2. Тогда n ≥ 2h/2 − 1
Логарифмируя, получим log2(n+1) ≥ h/2, или h ≤ 2 log2(n+1).

Операции:

    • Search,
    
    • Minimum,
    
    • Maximum,
    
    • Successor,
    
    • Predecessor
    
Ничем не отличаются от этих операций в обычном дереве поиска.
Сложность этих операций O(h) = O(2 log2(n+ 1)) = O(log n)

7. AVL-деревья. Основные операции над AVL-деревьями.

АВЛ-дерево — сбалансированное по высоте двоичное дерево поиска: для каждой его вершины высота её двух поддеревьев различается не более чем на 1.
АВЛ — аббревиатура, образованная первыми буквами фамилий создателей (советских учёных) Георгия Максимовича Адельсон-Вельского и Евгения Михайловича Ландиса.

Максимальная высота АВЛ-дерева при заданном числе узлов: h <= [1.45log2(n+2)]

[Больше информации](https://ru.wikipedia.org/wiki/АВЛ-дерево)

8. Косые деревья (Splay tree). Основные операции над косыми деревьями. Основные теоремы о косых деревьях. 

Косое дерево (splay tree) — это двоичное дерево поиска, основные операции над которым модифицируют структуру дерева таким образом, чтобы элемент, к которому осуществлялся доступ последний раз оказался корнем дерева.

Перекос — операция изменения структуры дерева, так что определенная вершина становится корнем дерева.

Основные операции: zig, zig-zig, zig-zag.

[Основные теоремы о косых деревьях. ](https://docviewer.yandex.ru/view/439651512/?page=14&*=pHQCNZfneLT919ErArXVTfVcMN97InVybCI6InlhLWRpc2stcHVibGljOi8vdG80dTd1T3dEOU9HMUI1aDg5bUcvbmgzdVU5Q0FxMUswVkpBbXR1dEZPRXRjSjdjNEFsMU81eFFxSHJ2YzU5VXEvSjZicG1SeU9Kb25UM1ZvWG5EYWc9PTov0KHQtdC80LXRgdGC0YAgNS%2FQkNC70LPQvtGA0LjRgtC80Ysg0Lgg0YHRgtGA0YPQutGC0YPRgNGLINC00LDQvdC90YvRhS%2FQm9C10LrRhtC40Lgg0YHQu9Cw0LnQtNGLL0FsZ29yaXRobXMgKDUpLnBkZiIsInRpdGxlIjoiQWxnb3JpdGhtcyAoNSkucGRmIiwibm9pZnJhbWUiOmZhbHNlLCJ1aWQiOiI0Mzk2NTE1MTIiLCJ0cyI6MTYxMDc4NjMwNTc4OCwieXUiOiIxNjUxMTQ0ODgxNTg0OTU1MjI3In0%3D)

9. Двоичные кучи. Очереди с приоритетами.

Двоичная куча — это двоичное дерево, хранящееся в массиве. Для двоичной кучи должны выполняться специальные свойства упорядоченности.
Каждая вершина дерева соответствует элементу массива: A[1], . . . , A[heapsize]. Корень имеет индекс 1
У вершины i есть родитель bi/2c и дети 2i и 2i+ 1. Параметры: длина массива length и размер кучи: heap-size.

Все уровни в дереве заполнены полностью. Поэтому высота дерева: Θ(log n)

Для каждой вершины с индексом i, кроме корня (при 2 ≤ i ≤ heapsize), выполняется: A[Parent(i)] > A[i] (Если наоборот — выполняется A[Parent(i)] 6 A[i], то куча называется минимальной кучей — MinHeap). Следовательно, значение потомка не превосходит значения предка.

Heapify — вспомогательный алгоритм, получающий на вход массив A и индекс i и делающий так, чтобы основное свойство кучи для поддерева с корнем i выполнялось. Сложность O(log n). Предполагается, что поддеревья с корнями Left(i) и Right(i) обладают основным свойством кучи.

Сложность построения кучи: O(n)

Добавление элемента: O(logn)

Очередь с приоритетами (priority queue) — это множество S, элементами которого являются пары 〈key, α〉, где key — число, определяющее приоритет элемента, а α — связанная с ним информация. Для простоты изложения, мы будем считать, что элементами множества являются только ключи. При извлечении элемента из очереди, каждый раз извлекается элемент с наибольшим приоритетом.

Реализация основана на куче. Для добавления элемента используется алгоритм HeapInsert. Для извлечения элемента — алгоритм HeapExtractMax. Обе эти операции требуют времени O(log n).

10. Алгоритм пирамидальной сортировки.  

Пирамидальная сортировка(Heap sort) — алгоритм сортировки, работающий в худшем, в среднем и в лучшем случае (то есть гарантированно) за Θ(n log n) операций при сортировке n элементов. Количество применяемой памяти не зависит от размера массива (то есть, O(1)).

Алгоритм:

    1) Построить max-кучу из данного масиссива
    
    2) Поменять head и tail местами, удалить tail из кучи.
    
    3) Повторить шаг 2 пока размер кучи болье 1.
    
Анализ алгоритма:
Достоинства

    • Имеет доказанную оценку худшего случая {\displaystyle O(n\cdot \log n)}O(n\cdot \log n).
    • Сортирует на месте, то есть требует всего O(1) дополнительной памяти.
    
Недостатки

    • Неустойчив — для обеспечения устойчивости нужно расширять ключ.
    • На почти отсортированных массивах работает столь же долго, как и на хаотических данных.
    • На одном шаге выборку приходится делать хаотично по всей длине массива — поэтому алгоритм плохо сочетается с кэшированием и подкачкой памяти.
    • Методу требуется «мгновенный» прямой доступ; не работает на связанных списках и других структурах памяти последовательного доступа.
    • Не распараллеливается.

11. Алгоритм сортировки слиянием. 

Хороший пример использования принципа «разделяй и властвуй». Сначала задача разбивается на несколько подзадач меньшего размера. Затем эти задачи решаются с помощью рекурсивного вызова или непосредственно, если их размер достаточно мал. Наконец, их решения комбинируются, и получается решение исходной задачи.

Алгоритм: 

Сортируемый массив разбивается на две части примерно одинакового размера;
Каждая из получившихся частей сортируется отдельно, например — тем же самым алгоритмом;
Два упорядоченных массива половинного размера соединяются в один.
1.1. — 2.1. Рекурсивное разбиение задачи на меньшие происходит до тех пор, пока размер массива не достигнет единицы (любой массив длины 1 можно считать упорядоченным).

3.1. Соединение двух упорядоченных массивов в один.
Основную идею слияния двух отсортированных массивов можно объяснить на следующем примере. Пусть мы имеем два уже отсортированных по возрастанию подмассива. Тогда:
3.2. Слияние двух подмассивов в третий результирующий массив.
На каждом шаге мы берём меньший из двух первых элементов подмассивов и записываем его в результирующий массив. Счётчики номеров элементов результирующего массива и подмассива, из которого был взят элемент, увеличиваем на 1.
3.4. «Прицепление» остатка.
Когда один из подмассивов закончился, мы добавляем все оставшиеся элементы второго подмассива в результирующий массив.

Достоинства:

Работает даже на структурах данных последовательного доступа.
Хорошо сочетается с подкачкой и кэшированием памяти.
Неплохо работает в параллельном варианте: легко разбить задачи между процессорами поровну, но трудно сделать так, чтобы другие процессоры взяли на себя работу, в случае если один процессор задержится.
Не имеет «трудных» входных данных.
Устойчивая - сохраняет порядок равных элементов (принадлежащих одному классу эквивалентности по сравнению).
Недостатки:

На «почти отсортированных» массивах работает столь же долго, как на хаотичных. Существует вариант сортировки слиянием, который работает быстрее на частично отсортированных данных, но он требует дополнительной памяти, в дополнении ко временному буферу, который используется непосредственно для сортировки.
Требует дополнительной памяти по размеру исходного массива.

12. Алгоритм быстрой сортировки.  
13. Алгоритм арифметической сортировки (сортировки подсчетом).

Алгоритм сортировки, в котором используется диапазон чисел сортируемого массива (списка) для подсчёта совпадающих элементов. Применение сортировки подсчётом целесообразно лишь тогда, когда сортируемые числа имеют (или их можно отобразить в) диапазон возможных значений, который достаточно мал по сравнению с сортируемым множеством, например, миллион натуральных чисел меньших 1000.

Алгоритм:

    1) Создадим вспомогательный массив C с индексами от 0 до k. Пройдемся по исходному массиву А и запишем в C[i] количество элементов, равных i.

    2) Массив C превратим в массив индексов (P=(1,0,3,3) ® P=(1,1,4,7))

    3) Создадим массив B, в котором будет храниться отсортированный массив.

    4) Заполним массив В.

Алгоритм имеет сложность не больше, чем O(n+k)

14. Понятие о гибридных алгоритмах сортировки. Алгоритм Introsort.

Гибридные сортировки – сортировки, в которых используются сразу несколько известных алгоритмов сортировки.

Introsort. Он использует быструю сортировку и переключается на пирамидальную сортировку, когда глубина рекурсии превысит некоторый заранее установленный уровень (например, логарифм от числа сортируемых элементов). Этот подход сочетает в себе достоинства обоих методов с худшим случаем O(n log n) и быстродействием, сравнимым с быстрой сортировкой. Так как оба алгоритма используют сравнения, этот алгоритм также принадлежит классу сортировок на основе сравнений.

15. Основные свойства алгоритмов сортировки. Нижняя оценка сложности сортировки сравнениями. 

К основным свойствам алгоритмов сортировки относятся:

    1) Сложность – функция зависимости объема работы, выполняемой некоторым алгоритмом, от размера входных данных. Асимптотически зависит от входа.

    2) Память – на месте или требуется выделение дополнительной памяти.

    3) Принцип – сравнение или нет.

    4) Устойчивость – не изменяет порядок элементов с одинаковыми ключами.

    5) Адаптивность – быстрее для частично отсортированных/ отсортированных массивов.

    6) Способность к распараллеливанию.

Теорема (о нижней оценке для сортировки сравнениями): высота любого решающего дерева, сортирующего n элементов, есть Ω(n log n).

Теорема: n! ≥ (n/e)^n.

Док-во: ln𝑛!= ∑ln𝑘 ≥∫𝑙𝑛𝑥𝑑𝑥=𝑛 𝑙𝑛 𝑛−𝑛+1𝑛1𝑛𝑘=1, т.к. ∫𝑙𝑛𝑥𝑑𝑥=𝑥 𝑙𝑛 𝑥−𝑛1𝑥+𝐶. Þ 𝑛!≥𝑒𝑛𝑛𝑒𝑛≥(𝑛/𝑒)𝑛

Среди листьев сортирующего дерева должны быть представлены все перестановки n элементов, число листьев не менее n!. Двоичное дерево высоты h имеет не более 2h листьев, имеем n!≤2h . Учитывая, что 𝑛!~√2𝜋𝑛(𝑛/𝑒)𝑛 , получаем (n/e)n≤2h. Логарифмируя, получаем: h ≥ n log n − n log e = Ω(n log n).


Следствие. Пирамидальная сортировка и сортировка слиянием асимптотически оптимальны.

16. Амортизационный анализ.

Амортизационный анализ (англ. amortized analysis) — метод подсчёта времени, требуемого для выполнения последовательности операций над структурой данных. При этом время усредняется по всем выполняемым операциям, и анализируется средняя производительность операций в худшем случае.

Такой анализ чаще всего используется, чтобы показать, что даже если некоторые из операций последовательности являются дорогостоящими, то при усреднении по всем операциям средняя их стоимость будет небольшой за счёт низкой частоты встречаемости. Подчеркнём, что оценка, даваемая амортизационным анализом, не является вероятностной: это оценка среднего времени выполнения операций для худшего случая.

Амортизационный анализ использует следующие методы:

    • Метод усреднения (метод группового анализа).
    • Метод потенциалов.
    • Метод предоплаты (метод бухгалтерского учёта).
    
    1. В методе усреднения амортизационная стоимость операций определяется напрямую по формуле: суммарная стоимость всех операций алгоритма делится на их количество.

    2. Введём для каждого состояния структуры данных величину Φ — потенциал. Изначально потенциал равен Φ0, а после выполнения i-й операции — Φi. Стоимость i-й операции обозначим ai=ti+Φi−Φ(i−1). Пусть n — количество операций, m — размер структуры данных. Тогда средняя амортизационная стоимость операций a=O(f(n,m)), если выполнены два условия: Для любого i:ai=O(f(n,m)), Для любого i:Φi=O(n⋅f(n,m))
    
    3. В методе предоплаты каждому типу операций присваивается своя учётная стоимость. Эта стоимость может быть больше фактической, в таком случае лишние монеты используются как резерв для выполнения других операций в будущем, а может быть меньше, тогда гарантируется, что текущего накопленного резерва достаточно для выполнения операции. Для доказательства оценки средней амортизационной стоимости O(f(n,m)) нужно построить учётные стоимости так, что для каждой операции она будет составлять O(f(n,m)). Тогда для последовательности из n операций суммарно будет затрачено n⋅O(f(n,m)) монет, следовательно, средняя амортизационная стоимость операций будет a=∑i=1ntin=n⋅O(f(n,m))n =O(f(n,m)).

17. Динамическое программирование.

Динамическое программирование — метод решения задачи путём её разбиения на несколько одинаковых подзадач, рекуррентно связанных между собой. Он применим к задачам с оптимальной подструктурой, выглядящим как набор перекрывающихся подзадач, сложность которых чуть меньше исходной. В этом случае время вычислений, по сравнению с «наивными» методами, можно значительно сократить.

Самым простым примером будут числа Фибоначчи — чтобы вычислить некоторое число в этой последовательности, нам нужно сперва вычислить третье число, сложив первые два, затем четвёртое таким же образом на основе второго и третьего, и так далее.

Необходимо наличие значений начальных состояний. В результате долгого разбиения на подзадачи необходимо свести функцию либо к уже известным значениям (как в случае с Фибоначчи — заранее определены первые два члена), либо к задаче, решаемой элементарно.

Ярким примером является вычисление последовательности Фибоначчи, {\displaystyle F_{3}=F_{2}+F_{1}}F_{3}=F_{2}+F_{1} и {\displaystyle F_{4}=F_{3}+F_{2}}F_{4}=F_{3}+F_{2} — даже в таком тривиальном случае вычисления всего двух чисел Фибоначчи мы уже посчитали {\displaystyle F_{2}}F_{2} дважды. Если продолжать дальше и посчитать {\displaystyle F_{5}}F_{5}, то {\displaystyle F_{2}}F_{2} посчитается ещё два раза, так как для вычисления {\displaystyle F_{5}}F_{5} будут нужны опять {\displaystyle F_{3}}F_{3} и {\displaystyle F_{4}}F_{4}. Получается следующее: простой рекурсивный подход будет расходовать время на вычисление решения для задач, которые он уже решал.

Чтобы избежать такого хода событий, мы будем сохранять решения подзадач, которые мы уже решали, и когда нам снова потребуется решение подзадачи, мы вместо того, чтобы вычислять его заново, просто достанем его из памяти. Этот подход называется мемоизацией. Можно проделывать и дальнейшие оптимизации — например, если мы точно уверены, что решение подзадачи нам больше не потребуется, можно выкинуть его из памяти, освободив её для других нужд, или если процессор простаивает и мы знаем, что решение некоторых, ещё не посчитанных подзадач, нам понадобится в дальнейшем, мы можем решить их заранее.

Подводя итоги вышесказанного можно сказать, что динамическое программирование пользуется следующими свойствами задачи:

    • Перекрывающиеся подзадачи;
    • Оптимальная подструктура;
    • Возможность запоминания решения часто встречающихся подзадач.

18. Жадные алгоритмы.

Жадный алгоритм (greedy algorithm) — это алгоритм, который на каждом шагу делает локально наилучший выбор в надежде, что итоговое решение будет оптимальным.

К примеру, алгоритм Дейкстры нахождения кратчайшего пути в графе вполне себе жадный, потому что мы на каждом шагу ищем вершину с наименьшим весом, в которой мы еще не бывали, после чего обновляем значения других вершин. При этом можно доказать, что кратчайшие пути, найденные в вершинах, являются оптимальными.

Говорят, что к оптимизационной задаче применим принцип жадного выбора, если последовательность локально оптимальных выборов даёт глобально оптимальное решение. В типичном случае доказательство оптимальности следует такой схеме:

Доказывается, что жадный выбор на первом шаге не закрывает пути к оптимальному решению: для всякого решения есть другое, согласованное с жадным выбором и не хуже первого. Показывается, что подзадача, возникающая после жадного выбора на первом шаге, аналогична исходной. Рассуждение завершается по индукции.

19. Расстояние Левенштейна. Алгоритм вычисления расстояния Левенштейна. 

Расстояние Левенштейна – это минимальное количество элементарных операций, необходимых для превращения одной строки в другую.
Операции: добавление, удаление, замена.

Свойства расстояния Левенштейна:

    1. d(S1, S2) >= || S1 - S2||
    2. d(S1, S2) <= max (|S1|, |S2|)
    3. d(S1, S2) = 0 => S1 = S

20. Вычислительно сложные задачи и основные подходы к их решению. 
21. Оптимизация перебора. Метод ветвей и границ. 
22. Приближенные алгоритмы. Приближенные алгоритмы для задачи о вершинном покрытии и задачи о рюкзаке.  
23. Эвристические алгоритмы. Локальный поиск. Алгоритм имитации отжига. Генетические алгоритмы.  
24. Задача о рюкзаке. Точные методы решения задачи о рюкзаке. 
25. Задача о рюкзаке. Приближенные методы решения задачи о рюкзаке. 
26. Вероятностные алгоритмы. Проверка на простоту на основе малой теоремы Ферма. 
27. Вероятностные алгоритмы. Фильтр Блума. Алгоритм MinHash. 
28. Вероятностные алгоритмы. Фильтр Блума. Алгоритм HyperLogLog. 
29. Рекурсия. Виды рекурсии. Оптимизация хвостовой рекурсии.

Рекурсия – вызов функции (процедуры) из неё же самой, непосредственно (простая рекурсия) или же через другие функции (сложная или косвенная или множественная рекурсия).Например: функция A вызывает функцию B, а функция B – функцию А. Так же рекурсия имеет условие выхода(останова).

Рекурсивные функции используют так называемый «Стек вызовов» (стек — это область памяти программы, где хранятся локальные переменные функций, а также адреса возврата из них). Когда программа вызывает функцию, функция отправляется на верх стека вызовов. Количество вложенных вызовов функции называется глубиной рекурсии.

Хвостовая рекурсия — частный случай рекурсии, при котором любой рекурсивный вызов является последней операцией перед возвратом из функции. Подобный вид рекурсии примечателен тем, что может быть легко заменён на итерацию путём формальной и гарантированно корректной перестройки кода функции. Оптимизация хвостовой рекурсии путём преобразования её в плоскую итерацию реализована во многих оптимизирующих компиляторах. В некоторых функциональных языках программирования спецификация гарантирует обязательную оптимизацию хвостовой рекурсии.

30. [Поиск наибольшей общей подпоследовательности](https://ru.wikipedia.org/wiki/Наибольшая_общая_подпоследовательность)